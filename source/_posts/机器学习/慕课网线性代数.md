---
tags:
  - 线性代数
categories:
  - 机器学习
cover: true
coverImg: /medias/文章图片/0.jpg
img: /medias/featureimages/99.jpg
summary: ' 线性代数在机器学习中至关重要，本文力图以几何视角再探线性代数，祝你好运！'s
abbrlink: 1024
---
概述：

1、线性代数研究一组数，即向量，以及向量的“函数”-矩阵；

//一组数更能真实模拟与描述我们的三维世界，单变量函数不精确；

2、机器学习学习方法：不一定要学透数学再学机器学习，

可以先实战、先入门然后有目的的补数学，带着目标学习；

3、机器学习需要：高数、线代、统计学（最重要）、凸优化；

//花书：深度学习圣经：

http://www.deeplearningbook.org/；

4、一手资源几乎全是英语，建议平时加强英语学习；

>   向量

1、向量：

① 起源：表示方向的有向线段，物理中速度、加速度；

② 与起点无关，默认从0,0开始；

③ 向量=点/有向线段；

④ 分类；行、列向量；

⑤ 向量加法：先走a在走b等价于先走b后走a；

//向量（5,2）先向x走5再向y走2；

⑥ 计算机科学常用证明方法：反证法、归纳法；

⑦ 向量内积：三角形借助余弦定理求出,定义为内积;

// 内积几何意义：投影

⑧ 点乘的应用：判断两个向量的相似程度（推荐系统）：

//推荐相似的物品，每一个物品都是高位空间的点（电影价格、导演、评分、主演）

每两个电影有夹角，锐角---相似；垂直：无关；

如果点乘越大正：完全重合；

>   矩阵

1、矩阵的几种理解：

① 矩阵=数表，工资表;

② 矩阵=线性系统（线性方程组），

如经济系统，网络中交通网络、信息网络，电路系统中电阻等式；化学系统中等式问题；

③ 矩阵乘法：行点乘，列组合；

④ 矩阵乘法：相当于向量的函数；

⑤ 矩阵=空间

\--\>\>列向量-第一列=基坐标1，第二列=基坐标2，张成新空间；

//表示新的空间中点的新坐标；

⑥ 矩阵=线性变换--恒纵坐标拉伸旋转变换；

\--\>可以直接写出翻转旋转的空间坐标（不能表示平移变换，因此引入仿射变换概念）；

⑦ 初等矩阵=初等变换；

2、求解矩阵的逆：

方法：A/E初等行变换，

电算步骤：高斯（从上到下）-约旦（从下至上）方法；

//本质：高斯-约旦消元法等价于找到一系列初等矩阵使得：

E1\*E2\*EK=A；

所以：求逆矩阵=求逆变换；

3、矩阵的逆重要性：

① ，如果A可逆直接求出,

//特别适合于：A不变，b有很多个的情况，此时可以复用；

4、矩阵的分解：LU分解

① 分解目的：提高计算效率；

② 含义： L：单位下三角-主对角线为1；

>   U：上三角；

③方法：高斯消元；

//高斯消元本质：初等变换为一个上三角矩阵；

，;；

④时间复杂度：O（）（即求解L矩阵的次数）

证明：左下方：

第一列下n-1个数字，每个数字需要操作n个数字（一行所有元素都要变化）

所以：(1+2+n-1)\*n=O（）;

⑤ 作用：求解线性方程组：

第一步：LUx=b,Ly=b先求出y，此时复杂度为O(n\^2/2)，

（因为下三角且为1每行只需要操作1所在元素，其他全为0）；

第二步：再次求解Ux=y，利用转置（或者约旦消元）可以求得（O（n\^2））；

\--T=O(N\^3/2)+2\*O(N\^2)

⑥ 高效证明：

求解逆矩阵：利用初等变换：为：

[2n\*（n-1）+2n\*1]\*2=O(2N\^3);

在加上A\^-1\*b需要O(2N\^2)

T2=O(2N\^3)+O(2N\^2)；

所以LU分解性能优化；

⑦ 其他分解：

Case1：由于U主对角线不为1，有时候不方便，因此考虑LDU分解，也就是把U初等变换为单位上三角；

Case2：如果不能进行LU分解，那么分解为PLU矩阵，将L换成PL，处理行交换矩阵；

\--P为置换矩阵；

>   向量空间

1、空间=一个集合；

2、特殊的向量空间：

欧几里得空间（）：有序实数元祖的集合（点集集合），n维空间的点；

如(5,66),(4,34,77.67)；

3、向量空间：元素为"向量"的集合（空间）；

什么是向量：不只是欧几里得中的有向点，定义两种运算：加法与数量乘法--\>\>

同时还需要满足十条性质：

①封闭性：ku+v属于向量空间V（满足加法和数乘封闭）；

//例如整数对于加法封闭，除法不封闭；

② 交换律、结合律、存在0向量属于向量空间；

什么是向量空间：欧几里得空间R\^N+其他空间；

广义向量空间：{所有2\*2方阵，所有m\*n矩阵；

>   所有多项式构成向量空间；所有某类函数构成向量空间}；

反例

//所有这种矩阵不组成向量空间；

4、维度：一个空间的极大无关组（基）的向量的个数；

//元素有几个数字，个数就是维度（错误:应当是极大无关组）；

特别的三维空间的过原点的平面，定义其维度为2；

5、

① 行空间为n维空间的子集；列空间为m维空间的子集；

② 求法：高斯行变换为最简形的非0行数=行秩=行空间的维度；

③ 秩为矩阵的秩，维度为向量空间的维度；

④ 同理列空间的极大无关组一般通过初等行变换得到RREF型求解；

>   列空间的基为化简后对应的原来的矩阵的列；

⑤ 区分向量空间的维度和向量的维度；

⑥ Ra行=Rb列；所以行空间与列空间的维度相等；

6、零空间：

① 定义：齐次线性方程组的所有解向量形成一个向量空间，称为0空间；

矩阵A的零空间就是AX=0中所有x组成的空间；

② 深入理解0空间：

>   view1：A=函数变换：所有向量在矩阵A变换下映射到零点；

view2：A=系统：Ax=0所有解组成的空间；

view3：A=空间：零空间是一个集合，

其中所有向量与A行向量点乘为0,A的0空间正交于A的行空间；

③
空间正交：任意两个向量垂直；如直线垂直平面，但是平面垂直平面不行（公共线不垂直）

\---两个二维平面（空间）在三维空间不正交，在4维正交；

④ 0空间的维度？

n-RA:原因：非自由列有ra个，剩下的为自由列，也就是解的列空间个数；

秩+零化度（0空间的维度）=n

7、子空间：

① 定义：向量空间V的子集S并且S还是一个向量空间；

或者：S为V空间：S为向量空间V子集同时对加法、数乘封闭；

如平面xoy上过原点的直线（不能是射线）为其子空间，不过原点就不是子空间（不包含0向量）；

② 举例：

如三维空间，原点，过原点直线，过原点平面都是子空间；

\--推而广之n为空间；

③ 子空间关系：

列空间：col(A) r=a

行空间：col（A\^T）r=ra

右0空间：NULL（A）r=n-ra

//正交于行空间；

左0空间：Null（A\^T）r=n-ra

//正交于列空间；

④ 子空间作用：降维：

AX=b，方程组个数太多（采集很多样本）很容易无解；

于是对于Ax（表示矩阵A的列（向量组成的）空间）再取求解离A列空间离b最近的b’,求解Ax=b';

//这就是最小二乘法的思路！

>   施密特正交化

1、二维情形：

问题：

已知u，v求垂直于u的向量

解法：

v垂直于u的投影向量可以求出为p，那么v-p向量必然垂直于u

所以：

就是所求的垂直向量；

2、三维：

a,b,c：任选两个a,b--u,v施法正交，第三个c投影到二维平面(uov)求第三个基，为方便求解，根据立体几何知识只需要在c投影到u,v二轴，求出投影向量k，然后c-k=w即可；

>   分解

1、矩阵QR分解：

① 含义：Q:标准正交矩阵；R（上三角矩阵）

② 用途：解方程组：

因为右边好求，R为三角阵也好求，所以简化线性方程组求解办法；

③ 如何求解Q,R?

方法1：求法：借助施法正交等式，求出与的关系；

A=(a1,a2,an)，施法正交为p1,p2,pn,规范化为q1,q2,qn

那么：(p1)\*q1=p1=a1---a1=r11p1;

(p2)\*q2=p2=a2-a2\*p1/(pi\^2)\*p1---a2=r21q1+r22q2;

所以：

方法2：实际使用的最多还是：

>   基、坐标变换

1、点的坐标以一组基为标准，

（

坐标的值为对应基的线性组合的系数；

2、标准基:

含义：

即:标准坐标系；

3、标准正交基和标准基；

前者为单位正交向量组，后者特指标准坐标系的基向量；

4、坐标系转换：

桥梁：，（直观理解：条条大路走到该点）；

举例：

>   线性变换

1、线性变换：

定义：

条件：

//特别的：在欧几里得空间中，矩阵=线性变换；

>   此时：；

2、不同维度空间的变化：3D到2D动画；计算机视觉：2D-\>3D；

同维度：数据压缩---新的基中y方向差别很小，降维为x一维，

如jpeg、傅里叶变换都是找新基，得到新特征；

>   行列式

1、行列式：方阵的一个属性；

行列式表示向量组在空间的有向体积！

2、行列式电算：

高斯消元法化为上三角（注意不能倍乘、交换），主对角线出现0就是0；

特征值，特征向量：

1、特征值、特征向量：方阵的一个属性；

① 定义：

② 特征空间即：{}∪{λ的特征向量}，即A-λI的零空间；

2、投影变换：

① 投影变换为线性变换，对应一个矩阵，即：矩阵=投影变换=线性变换；

特征向量：经过投影变换后的向量与原来的向量在同一直线的向量；

如

表示翻转变化，关于y=x对称，此时特征向量为y=x上向量，对应特征值为1，；

而y=-x上向量翻转后对应λ为-1的特征向量；

② 复数：

基坐标来看是旋转，直观理解没有特征向量，更不用说特征值，求出λ为复数；

③ 任意：

单位矩阵：A=I，特征值为1，特征向量为任何向量，基为；

④ 代数重数大于等于几何重数（特征空间的维度）；

3、矩阵相似：；

① 几何解释：P是一个坐标系，A变换是P坐标系下观察的B变换；

② 类似于相似三角形；

③ A和B本质是一个变换，只不过观察的坐标系不同；

④ 特例：表示A变换在P坐标系下观察到的变换；

//应用：求方阵的幂；

⑤ 为什么求矩阵的幂？

动态系统：中的特征值反应系统各个分量的变化速率；

>   对称阵与奇异值

1、对称阵：;

① 特征值一定是实数！并且一定可以相似对角化--几何重数一定等于代数重数；

② 不同特征值对应特征向量正交；

③ 对称阵一定可以正交变换为对角阵；

2、奇异值：如果有，那么为对称方阵，可正交对角化

有（已规范化）

其中为A的i列\*A的j列；

那么：

① ，表示矩阵的奇异值；

② 奇异值表示即向量的长度；

③ 表示的列空间的一组正交基，；

④ 注意事项：奇异值是对于矩阵而言，特征值是对于而言，

A的奇异值为的特征值开根号！

⑤ 如果有个不等于0，

那么组成的列空间的一组正交基，

>   且：，

那么：表示的列空间的一组标准正交基；

3、分解：对矩阵没有限制

① 矩阵的奇异值分解（不是矩阵的分解）：

② 形式：

其中：，为标准正交阵；

是的特征向量进行标准化之后的矩阵（标准正交矩阵）；

Σ：奇异值矩阵：

其中：；

③ 证明

④ 算法过程：

S1:求解特征值、特征向量；

S2:奇异值从大到小排序，得到Σ；

S3:特征向量标准化得到矩阵；

S4:求出（≠0），然后施法拓展；

⑤ 分解应用：

应用1： 对一个维列向量变换：

几何意义：原来的在基坐标下坐标为，经过变换后，在坐标系下为对进行奇异值倍拉伸；

举例：圆—变换为-椭圆；

应用2：

几何意义：矩阵可以看成多个递降权值的矩阵；

//因此可以进行（图像）矩阵压缩、降噪、降维，保留大的奇异值的矩阵；

应用3： 应用于推荐系统、、搜索引擎；

4、最后展望；

1、统计学：PCA，最小二乘法；

微积分：差分方程、微分方程；

2、机器学习、推荐系统、图像学、经济学；

3、对领域感兴趣去学习，再补数学，有目的；
